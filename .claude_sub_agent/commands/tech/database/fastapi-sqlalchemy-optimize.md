# /fastapi-sqlalchemy-optimize - ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰æœ€é©åŒ–

## æ¦‚è¦
FastAPI + SQLAlchemy + PostgreSQLã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚·ã‚¹ãƒ†ãƒ ã®åŒ…æ‹¬çš„æœ€é©åŒ–ã‚³ãƒãƒ³ãƒ‰ã§ã™ã€‚**ã‚·ã‚¹ãƒ†ãƒ é–‹ç™ºéƒ¨ä¸»å°**ã§éƒ¨é–€å”èª¿ã«ã‚ˆã‚‹é«˜æ€§èƒ½åŒ–ãƒ»è¦åˆ¶å¯¾å¿œãƒ»çµ±åˆåŒ–ã‚’å®Ÿç¾ã—ã¾ã™ã€‚

## ğŸ¯ éƒ¨é–€åˆ¥è²¬ä»»åˆ†æ‹…

### ã‚·ã‚¹ãƒ†ãƒ é–‹ç™ºéƒ¨ï¼ˆå®Ÿè£…è²¬ä»»ï¼‰
- FastAPIæœ€é©åŒ–ãƒ»éåŒæœŸå‡¦ç†æ”¹å–„
- SQLAlchemy ORMæœ€é©åŒ–ãƒ»ã‚¯ã‚¨ãƒªãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å®Ÿè£…
- APIè¨­è¨ˆãƒ»ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆæœ€é©åŒ–

### å“è³ªä¿è¨¼éƒ¨ï¼ˆæ¤œè¨¼è²¬ä»»ï¼‰
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆãƒ»è² è·ãƒ†ã‚¹ãƒˆ
- ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§æ¤œè¨¼ãƒ»ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³æ¤œè¨¼
- ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆãƒ»è„†å¼±æ€§æ¤œæŸ»
- å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ç›£è¦–ãƒ»å›å¸°ãƒ†ã‚¹ãƒˆ

### çµŒå–¶ä¼ç”»éƒ¨ï¼ˆæˆ¦ç•¥è²¬ä»»ï¼‰
- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æˆ¦ç•¥
- ç›£è¦–ãƒ»å¯è¦³æ¸¬æ€§æˆ¦ç•¥
- ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£è¨­è¨ˆ
- ROIåŠ¹æœæ¸¬å®šãƒ»ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ

### äººäº‹éƒ¨ï¼ˆé‹ç”¨è²¬ä»»ï¼‰
- é‹ç”¨æ‰‹é †ãƒ»ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ä½œæˆ
- ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ»æŠ€è¡“ç¶™æ‰¿
- ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹ç®¡ç†
- éšœå®³å¯¾å¿œãƒ»é‹ç”¨ç›£è¦–

## ğŸš€ åŸºæœ¬ä½¿ç”¨æ³•

```bash
# éƒ¨é–€å”èª¿ã«ã‚ˆã‚‹åŒ…æ‹¬çš„æœ€é©åŒ–ï¼ˆæ¨å¥¨ï¼‰
/fastapi-sqlalchemy-optimize comprehensive

# ã‚·ã‚¹ãƒ†ãƒ é–‹ç™ºéƒ¨: APIãƒ»ORMæœ€é©åŒ–
/fastapi-sqlalchemy-optimize performance --focus="fastapi,sqlalchemy,async"

# å“è³ªä¿è¨¼éƒ¨: å“è³ªãƒ»ãƒ†ã‚¹ãƒˆæœ€é©åŒ–
/fastapi-sqlalchemy-optimize quality --focus="testing,security,validation"

# çµŒå–¶ä¼ç”»éƒ¨: ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãƒ»æˆ¦ç•¥æœ€é©åŒ–
/fastapi-sqlalchemy-optimize strategy --focus="architecture,monitoring,scaling"
```

## ğŸ“‹ æœ€é©åŒ–ã‚«ãƒ†ã‚´ãƒªãƒ¼

### 1. FastAPIæœ€é©åŒ–ï¼ˆã‚·ã‚¹ãƒ†ãƒ é–‹ç™ºéƒ¨ï¼‰

#### éåŒæœŸå‡¦ç†æœ€é©åŒ–
```python
from fastapi import FastAPI, Depends, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
from contextlib import asynccontextmanager
import asyncio
from typing import List, Optional
import uvloop

# é«˜æ€§èƒ½éåŒæœŸãƒ«ãƒ¼ãƒ—è¨­å®š
asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•æœ€é©åŒ–
@asynccontextmanager
async def lifespan(app: FastAPI):
    # èµ·å‹•æ™‚å‡¦ç†
    await initialize_database_pool()
    await setup_redis_connection()
    await start_background_services()
    
    yield
    
    # çµ‚äº†æ™‚å‡¦ç†
    await cleanup_database_pool()
    await close_redis_connection()

app = FastAPI(
    title="Enterprise API",
    version="2.0.0",
    lifespan=lifespan
)

# ãƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢æœ€é©åŒ–
app.add_middleware(GZipMiddleware, minimum_size=1000)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ¥ç¶šãƒ—ãƒ¼ãƒ«æœ€é©åŒ–
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from sqlalchemy.pool import NullPool

engine = create_async_engine(
    DATABASE_URL,
    poolclass=NullPool,  # PgBouncerä½¿ç”¨æ™‚
    pool_pre_ping=True,
    pool_recycle=3600,
    echo=False,
    future=True
)

AsyncSessionLocal = sessionmaker(
    engine, 
    class_=AsyncSession, 
    expire_on_commit=False
)

async def get_db():
    async with AsyncSessionLocal() as session:
        try:
            yield session
        finally:
            await session.close()

# é«˜é€Ÿãƒãƒ«ã‚¯æ“ä½œAPI
@app.post("/api/bulk-operations/")
async def bulk_create_items(
    items: List[ItemCreate],
    background_tasks: BackgroundTasks,
    db: AsyncSession = Depends(get_db)
):
    # ãƒãƒ«ã‚¯æŒ¿å…¥æœ€é©åŒ–
    items_data = [item.dict() for item in items]
    
    # å¤§é‡ãƒ‡ãƒ¼ã‚¿ã¯åˆ†å‰²å‡¦ç†
    if len(items_data) > 1000:
        background_tasks.add_task(process_bulk_items, items_data)
        return {"status": "accepted", "message": "Bulk processing started"}
    
    # å°é‡ãƒ‡ãƒ¼ã‚¿ã¯å³åº§ã«å‡¦ç†
    result = await db.execute(
        insert(Item).values(items_data)
    )
    await db.commit()
    
    return {"status": "completed", "inserted": len(items_data)}

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±åˆ
from functools import lru_cache
import redis.asyncio as redis

redis_client = redis.Redis(
    host='localhost',
    port=6379,
    decode_responses=True,
    max_connections=20
)

@lru_cache(maxsize=128)
def get_settings():
    return Settings()

async def cached_query(cache_key: str, query_func, ttl: int = 300):
    # Redis ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª
    cached_result = await redis_client.get(cache_key)
    if cached_result:
        return json.loads(cached_result)
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœªå‘½ä¸­æ™‚ã¯DBã‚¯ã‚¨ãƒªå®Ÿè¡Œ
    result = await query_func()
    
    # çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
    await redis_client.setex(
        cache_key, 
        ttl, 
        json.dumps(result, default=str)
    )
    
    return result
```

#### ãƒ¬ã‚¹ãƒãƒ³ã‚¹æœ€é©åŒ–
```python
from fastapi.responses import JSONResponse, StreamingResponse
from fastapi.encoders import jsonable_encoder
import orjson

# é«˜é€ŸJSON ãƒ¬ã‚¹ãƒãƒ³ã‚¹
class ORJSONResponse(JSONResponse):
    media_type = "application/json"

    def render(self, content) -> bytes:
        return orjson.dumps(content, default=str)

app = FastAPI(default_response_class=ORJSONResponse)

# ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹
@app.get("/api/large-dataset/")
async def stream_large_dataset(db: AsyncSession = Depends(get_db)):
    async def generate():
        async with db.stream(select(LargeTable)) as result:
            async for row in result:
                yield orjson.dumps(row._asdict()) + b'\n'
    
    return StreamingResponse(
        generate(),
        media_type="application/x-ndjson"
    )

# ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æœ€é©åŒ–
@app.get("/api/items/")
async def get_items(
    page: int = 1,
    size: int = 20,
    db: AsyncSession = Depends(get_db)
):
    offset = (page - 1) * size
    
    # ã‚«ã‚¦ãƒ³ãƒˆã‚¯ã‚¨ãƒªã¨ãƒ‡ãƒ¼ã‚¿ã‚¯ã‚¨ãƒªã‚’ä¸¦åˆ—å®Ÿè¡Œ
    count_task = asyncio.create_task(
        db.scalar(select(func.count(Item.id)))
    )
    items_task = asyncio.create_task(
        db.execute(
            select(Item)
            .offset(offset)
            .limit(size)
            .options(selectinload(Item.related_data))
        )
    )
    
    total, items_result = await asyncio.gather(count_task, items_task)
    items = items_result.scalars().all()
    
    return {
        "items": items,
        "total": total,
        "page": page,
        "pages": (total + size - 1) // size
    }
```

### 2. SQLAlchemy ORMæœ€é©åŒ–ï¼ˆã‚·ã‚¹ãƒ†ãƒ é–‹ç™ºéƒ¨ï¼‰

#### åŠ¹ç‡çš„ãƒ¢ãƒ‡ãƒ«è¨­è¨ˆ
```python
from sqlalchemy import Column, Integer, String, DateTime, ForeignKey, Index
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship, selectinload, joinedload
from sqlalchemy.dialects.postgresql import UUID, JSONB
import uuid

Base = declarative_base()

class OptimizedItem(Base):
    __tablename__ = "items"
    
    # æœ€é©åŒ–ã•ã‚ŒãŸãƒ—ãƒ©ã‚¤ãƒãƒªã‚­ãƒ¼
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    name = Column(String(255), nullable=False, index=True)
    category_id = Column(UUID(as_uuid=True), ForeignKey("categories.id"), index=True)
    
    # JSON ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æœ€é©åŒ–
    metadata = Column(JSONB, nullable=True)
    
    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
    created_at = Column(DateTime, default=func.now(), index=True)
    updated_at = Column(DateTime, default=func.now(), onupdate=func.now())
    
    # æœ€é©åŒ–ã•ã‚ŒãŸãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚·ãƒƒãƒ—
    category = relationship(
        "Category", 
        back_populates="items",
        lazy="select"
    )
    
    # è¤‡åˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    __table_args__ = (
        Index('ix_items_category_created', 'category_id', 'created_at'),
        Index('ix_items_name_category', 'name', 'category_id'),
    )

# ãƒªãƒã‚¸ãƒˆãƒªãƒ‘ã‚¿ãƒ¼ãƒ³å®Ÿè£…
class ItemRepository:
    def __init__(self, db: AsyncSession):
        self.db = db
    
    async def find_with_optimized_loading(
        self, 
        category_id: Optional[UUID] = None,
        limit: int = 100
    ) -> List[OptimizedItem]:
        query = select(OptimizedItem)
        
        if category_id:
            query = query.where(OptimizedItem.category_id == category_id)
        
        # æœ€é©åŒ–ã•ã‚ŒãŸeager loading
        query = query.options(
            selectinload(OptimizedItem.category)
        ).limit(limit)
        
        result = await self.db.execute(query)
        return result.scalars().all()
    
    async def bulk_update_optimized(
        self, 
        updates: List[dict]
    ) -> int:
        # ãƒãƒ«ã‚¯æ›´æ–°æœ€é©åŒ–
        if not updates:
            return 0
        
        stmt = update(OptimizedItem).where(
            OptimizedItem.id == bindparam('item_id')
        ).values(
            name=bindparam('name'),
            updated_at=func.now()
        )
        
        await self.db.execute(stmt, updates)
        await self.db.commit()
        
        return len(updates)
```

#### ã‚¯ã‚¨ãƒªæœ€é©åŒ–
```python
from sqlalchemy import select, func, and_, or_, case
from sqlalchemy.orm import aliased

class OptimizedQueryService:
    def __init__(self, db: AsyncSession):
        self.db = db
    
    async def complex_analytics_query(
        self, 
        start_date: datetime,
        end_date: datetime
    ) -> dict:
        # ã‚µãƒ–ã‚¯ã‚¨ãƒªæœ€é©åŒ–
        subquery = select(
            OptimizedItem.category_id,
            func.count(OptimizedItem.id).label('item_count'),
            func.sum(
                case(
                    (OptimizedItem.metadata['price'].astext.cast(Integer) > 100, 1),
                    else_=0
                )
            ).label('expensive_items')
        ).where(
            and_(
                OptimizedItem.created_at >= start_date,
                OptimizedItem.created_at <= end_date
            )
        ).group_by(OptimizedItem.category_id).subquery()
        
        # ãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªï¼ˆJOINæœ€é©åŒ–ï¼‰
        query = select(
            Category.name.label('category_name'),
            subquery.c.item_count,
            subquery.c.expensive_items,
            func.round(
                (subquery.c.expensive_items.cast(float) / subquery.c.item_count * 100), 2
            ).label('expensive_percentage')
        ).select_from(
            Category.__table__.join(
                subquery, 
                Category.id == subquery.c.category_id
            )
        ).order_by(subquery.c.item_count.desc())
        
        result = await self.db.execute(query)
        return [dict(row) for row in result]
    
    async def optimized_search(
        self, 
        search_term: str,
        filters: dict = None
    ) -> List[OptimizedItem]:
        # ãƒ•ãƒ«ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢æœ€é©åŒ–
        query = select(OptimizedItem)
        
        if search_term:
            # PostgreSQL ãƒ•ãƒ«ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢
            query = query.where(
                or_(
                    OptimizedItem.name.ilike(f'%{search_term}%'),
                    OptimizedItem.metadata['description'].astext.ilike(f'%{search_term}%')
                )
            )
        
        # å‹•çš„ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨
        if filters:
            for key, value in filters.items():
                if hasattr(OptimizedItem, key):
                    query = query.where(getattr(OptimizedItem, key) == value)
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ´»ç”¨ã®ç¢ºèª
        result = await self.db.execute(query)
        return result.scalars().all()
```

### 3. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–ï¼ˆçµŒå–¶ä¼ç”»éƒ¨ï¼‰

#### æ¥ç¶šãƒ—ãƒ¼ãƒ«ãƒ»ç›£è¦–è¨­å®š
```python
from sqlalchemy.pool import QueuePool
import logging

# æœ¬ç•ªç’°å¢ƒç”¨ã‚¨ãƒ³ã‚¸ãƒ³è¨­å®š
production_engine = create_async_engine(
    DATABASE_URL,
    poolclass=QueuePool,
    pool_size=20,
    max_overflow=30,
    pool_pre_ping=True,
    pool_recycle=3600,
    pool_timeout=30,
    echo=False,
    echo_pool=True,  # é–‹ç™ºæ™‚ã®ã¿
    future=True
)

# ç›£è¦–ã¨ãƒ­ã‚®ãƒ³ã‚°
logging.basicConfig(level=logging.INFO)
db_logger = logging.getLogger('sqlalchemy.engine')

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–
class DatabaseMonitor:
    def __init__(self):
        self.query_times = []
        self.slow_queries = []
    
    async def log_query_performance(self, query: str, duration: float):
        self.query_times.append(duration)
        
        if duration > 1.0:  # 1ç§’ä»¥ä¸Šã®ã‚¯ã‚¨ãƒª
            self.slow_queries.append({
                'query': query,
                'duration': duration,
                'timestamp': datetime.now()
            })
    
    def get_performance_stats(self) -> dict:
        if not self.query_times:
            return {}
        
        return {
            'avg_query_time': sum(self.query_times) / len(self.query_times),
            'max_query_time': max(self.query_times),
            'slow_queries_count': len(self.slow_queries),
            'total_queries': len(self.query_times)
        }
```

## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹

### ç›®æ¨™æŒ‡æ¨™
- **APIå¿œç­”æ™‚é–“**: å¹³å‡100msä»¥ä¸‹
- **ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒª**: å¹³å‡50msä»¥ä¸‹
- **ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ**: 1000 req/secä»¥ä¸Š
- **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡**: æœ€é©åŒ–ã«ã‚ˆã‚Š50%å‰Šæ¸›

### ç›£è¦–æŒ‡æ¨™
- **æ¥ç¶šãƒ—ãƒ¼ãƒ«ä½¿ç”¨ç‡**: 80%ä»¥ä¸‹
- **ã‚¹ãƒ­ãƒ¼ã‚¯ã‚¨ãƒªç‡**: 5%ä»¥ä¸‹
- **ã‚¨ãƒ©ãƒ¼ç‡**: 1%ä»¥ä¸‹
- **å¯ç”¨æ€§**: 99.9%ä»¥ä¸Š

## ğŸ”§ å®Ÿè£…æ‰‹é †

### Phase 1: ç¾çŠ¶åˆ†æï¼ˆå“è³ªä¿è¨¼éƒ¨ä¸»å°ï¼‰
```bash
# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
/analyze performance --scope="fastapi,sqlalchemy,database"

# ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®š
/analyze bottleneck --focus="api_endpoints,db_queries,connection_pool"
```

### Phase 2: æœ€é©åŒ–å®Ÿè£…ï¼ˆã‚·ã‚¹ãƒ†ãƒ é–‹ç™ºéƒ¨ä¸»å°ï¼‰
```bash
# FastAPIæœ€é©åŒ–
/enhance fastapi --focus="async,middleware,response_optimization"

# SQLAlchemyæœ€é©åŒ–
/enhance sqlalchemy --focus="orm,queries,relationships"

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–
/enhance database --focus="indexes,connection_pool,caching"
```

### Phase 3: ã‚¤ãƒ³ãƒ•ãƒ©æœ€é©åŒ–ï¼ˆçµŒå–¶ä¼ç”»éƒ¨ä¸»å°ï¼‰
```bash
# ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆ
/architecture backend --focus="scalability,monitoring,high_availability"

# ç›£è¦–ãƒ»å¯è¦³æ¸¬æ€§
/devops monitoring --focus="performance_metrics,alerting,logging"
```

### Phase 4: å“è³ªä¿è¨¼ï¼ˆå“è³ªä¿è¨¼éƒ¨ä¸»å°ï¼‰
```bash
# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
/test performance --scope="load,stress,endurance"

# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ
/test security --focus="api_security,data_protection"
```

## ğŸ¯ ç¶™ç¶šæ”¹å–„

### è‡ªå‹•åŒ–ç›£è¦–
- APMçµ±åˆï¼ˆNew Relicã€DataDogç­‰ï¼‰
- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç›£è¦–ãƒ»æœ€é©åŒ–ææ¡ˆ
- è‡ªå‹•ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ»è² è·åˆ†æ•£

### å®šæœŸæœ€é©åŒ–
- æœˆæ¬¡: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æãƒ»æ”¹å–„
- å››åŠæœŸ: ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¦‹ç›´ã—
- å¹´æ¬¡: æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯è©•ä¾¡ãƒ»æ›´æ–°

---

**ğŸ¯ ç›®æ¨™**: FastAPI + SQLAlchemyã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã§ã€æœ€é«˜ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¨ä¿¡é ¼æ€§ã‚’éƒ¨é–€å”èª¿ã«ã‚ˆã‚Šå®Ÿç¾ã™ã‚‹ã€‚