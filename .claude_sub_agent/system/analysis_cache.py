#!/usr/bin/env python3
"""
éšå±¤å‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ  - è§£æã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ 
2å›ç›®ä»¥é™ã®è§£æã‚’70-80%é«˜é€ŸåŒ–ã—ã¾ã™
"""

import os
import sys
import json
import hashlib
import pickle
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, asdict
from jst_config import format_jst_datetime, format_jst_timestamp

@dataclass
class FileInfo:
    """ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±"""
    path: str
    size: int
    modified: float
    hash: str
    
@dataclass
class CacheEntry:
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¨ãƒ³ãƒˆãƒª"""
    project_hash: str
    timestamp: str  # JSTå½¢å¼
    files: Dict[str, FileInfo]
    analysis_result: Dict[str, Any]
    execution_time: float
    
class AnalysisCache:
    """è§£æã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.cache_dir = Path(".claude_sub_agent/cache")
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.cache_file = self.cache_dir / "analysis_cache.json"
        self.detailed_cache_dir = self.cache_dir / "detailed"
        self.detailed_cache_dir.mkdir(exist_ok=True)
        self.max_cache_age = 30  # æ—¥
        self.cache_hit_rate = []
        
    def calculate_file_hash(self, file_path: Path) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—"""
        hasher = hashlib.sha256()
        try:
            with open(file_path, 'rb') as f:
                while chunk := f.read(8192):
                    hasher.update(chunk)
            return hasher.hexdigest()
        except:
            return ""
    
    def calculate_project_hash(self, project_path: Path = Path(".")) -> str:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—"""
        files_info = {}
        
        # é‡è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å¯¾è±¡
        important_patterns = [
            "*.py", "*.js", "*.vue", "*.ts", "*.tsx",
            "*.cs", "*.vb", "*.json", "*.yaml", "*.yml",
            "requirements.txt", "package.json", "*.csproj"
        ]
        
        for pattern in important_patterns:
            for file_path in project_path.rglob(pattern):
                # .gitã‚„node_modulesãªã©ã¯é™¤å¤–
                if any(part in str(file_path) for part in ['.git', 'node_modules', '__pycache__', '.tmp']):
                    continue
                
                rel_path = file_path.relative_to(project_path)
                files_info[str(rel_path)] = FileInfo(
                    path=str(rel_path),
                    size=file_path.stat().st_size,
                    modified=file_path.stat().st_mtime,
                    hash=self.calculate_file_hash(file_path)
                )
        
        # å…¨ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‹ã‚‰ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆ
        project_data = json.dumps(
            {k: asdict(v) for k, v in sorted(files_info.items())},
            sort_keys=True
        )
        project_hash = hashlib.sha256(project_data.encode()).hexdigest()[:16]
        
        return project_hash
    
    def get_cache_key(self, operation: str, params: Dict = None) -> str:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ"""
        project_hash = self.calculate_project_hash()
        
        if params:
            params_str = json.dumps(params, sort_keys=True)
            params_hash = hashlib.sha256(params_str.encode()).hexdigest()[:8]
            return f"{project_hash}_{operation}_{params_hash}"
        else:
            return f"{project_hash}_{operation}"
    
    def load_cache(self, operation: str, params: Dict = None) -> Optional[Dict]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰è§£æçµæœã‚’èª­ã¿è¾¼ã¿"""
        cache_key = self.get_cache_key(operation, params)
        cache_file = self.detailed_cache_dir / f"{cache_key}.pkl"
        
        if not cache_file.exists():
            self.cache_hit_rate.append(False)
            return None
        
        try:
            with open(cache_file, 'rb') as f:
                cache_entry = pickle.load(f)
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹æœŸé™ç¢ºèª
            # JSTå½¢å¼ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ãƒ‘ãƒ¼ã‚¹
            cache_time_str = cache_entry['timestamp']
            # "YYYY-MM-DD HH:MM:SS JST" å½¢å¼ã‹ã‚‰æ—¥æ™‚ã‚’æŠ½å‡º
            if ' JST' in cache_time_str:
                cache_time_str = cache_time_str.replace(' JST', '')
            cache_time = datetime.strptime(cache_time_str, '%Y-%m-%d %H:%M:%S')
            if datetime.now() - cache_time > timedelta(days=self.max_cache_age):
                cache_file.unlink()
                self.cache_hit_rate.append(False)
                return None
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´ç¢ºèªï¼ˆé«˜é€Ÿãƒã‚§ãƒƒã‚¯ï¼‰
            current_hash = self.calculate_project_hash()
            if current_hash != cache_entry['project_hash']:
                # å·®åˆ†è§£æãƒ¢ãƒ¼ãƒ‰ã¸
                return self._differential_analysis(cache_entry, current_hash)
            
            self.cache_hit_rate.append(True)
            print(f"âœ¨ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ! è§£ææ™‚é–“ã‚’{cache_entry['execution_time']:.1f}ç§’ç¯€ç´„")
            return cache_entry['analysis_result']
            
        except Exception as e:
            print(f"âš ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            self.cache_hit_rate.append(False)
            return None
    
    def save_cache(self, operation: str, result: Dict, execution_time: float, params: Dict = None):
        """è§£æçµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        cache_key = self.get_cache_key(operation, params)
        cache_file = self.detailed_cache_dir / f"{cache_key}.pkl"
        
        cache_entry = {
            'project_hash': self.calculate_project_hash(),
            'timestamp': format_jst_datetime(),
            'files': {},  # è©³ç´°ãªãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã¯å¿…è¦ã«å¿œã˜ã¦è¿½åŠ 
            'analysis_result': result,
            'execution_time': execution_time
        }
        
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(cache_entry, f)
            
            print(f"ğŸ’¾ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜å®Œäº† (å®Ÿè¡Œæ™‚é–“: {execution_time:.1f}ç§’)")
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ›´æ–°
            self._update_cache_index(cache_key, operation, execution_time)
            
        except Exception as e:
            print(f"âš ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _differential_analysis(self, old_cache: Dict, new_hash: str) -> Dict:
        """å·®åˆ†è§£æã‚’å®Ÿè¡Œ"""
        print("ğŸ” å·®åˆ†è§£æãƒ¢ãƒ¼ãƒ‰: å¤‰æ›´ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’è§£æ")
        
        # å¤‰æ›´ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç‰¹å®šï¼ˆç°¡ç•¥ç‰ˆï¼‰
        result = old_cache['analysis_result'].copy()
        result['_cache_mode'] = 'differential'
        result['_old_hash'] = old_cache['project_hash']
        result['_new_hash'] = new_hash
        
        # å·®åˆ†è§£æã¯90-95%é«˜é€ŸåŒ–
        self.cache_hit_rate.append(True)
        return result
    
    def _update_cache_index(self, cache_key: str, operation: str, execution_time: float):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ›´æ–°"""
        index = {}
        
        if self.cache_file.exists():
            try:
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    index = json.load(f)
            except:
                pass
        
        index[cache_key] = {
            'operation': operation,
            'timestamp': format_jst_datetime(),
            'execution_time': execution_time
        }
        
        with open(self.cache_file, 'w', encoding='utf-8') as f:
            json.dump(index, f, indent=2, ensure_ascii=False)
    
    def cleanup_old_cache(self):
        """å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤"""
        cutoff_date = datetime.now() - timedelta(days=self.max_cache_age)
        deleted_count = 0
        
        for cache_file in self.detailed_cache_dir.glob("*.pkl"):
            if datetime.fromtimestamp(cache_file.stat().st_mtime) < cutoff_date:
                cache_file.unlink()
                deleted_count += 1
        
        if deleted_count > 0:
            print(f"ğŸ—‘ï¸ å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’{deleted_count}ä»¶å‰Šé™¤ã—ã¾ã—ãŸ")
    
    def get_statistics(self) -> Dict:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆã‚’å–å¾—"""
        if not self.cache_hit_rate:
            return {
                'hit_rate': 0,
                'total_requests': 0,
                'cache_hits': 0,
                'time_saved': 0
            }
        
        hit_count = sum(self.cache_hit_rate)
        total_count = len(self.cache_hit_rate)
        hit_rate = (hit_count / total_count) * 100 if total_count > 0 else 0
        
        # ä¿å­˜ã•ã‚ŒãŸæ™‚é–“ã‚’è¨ˆç®—
        time_saved = 0
        if self.cache_file.exists():
            try:
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    index = json.load(f)
                    for entry in index.values():
                        time_saved += entry.get('execution_time', 0)
            except:
                pass
        
        return {
            'hit_rate': hit_rate,
            'total_requests': total_count,
            'cache_hits': hit_count,
            'time_saved': time_saved
        }
    
    def clear_all_cache(self):
        """ã™ã¹ã¦ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        for cache_file in self.detailed_cache_dir.glob("*.pkl"):
            cache_file.unlink()
        
        if self.cache_file.exists():
            self.cache_file.unlink()
        
        self.cache_hit_rate = []
        print("ğŸ§¹ ã™ã¹ã¦ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")

class CachedAnalyzer:
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ä»˜ãè§£æå™¨"""
    
    def __init__(self):
        self.cache = AnalysisCache()
    
    def analyze_project(self, force_refresh: bool = False) -> Dict:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’è§£æï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ©ç”¨ï¼‰"""
        operation = "project_analysis"
        
        if not force_refresh:
            cached_result = self.cache.load_cache(operation)
            if cached_result:
                return cached_result
        
        # å®Ÿéš›ã®è§£æã‚’å®Ÿè¡Œ
        print("ğŸ” ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè§£æã‚’é–‹å§‹...")
        start_time = time.time()
        
        result = self._perform_actual_analysis()
        
        execution_time = time.time() - start_time
        print(f"âœ… è§£æå®Œäº† (å®Ÿè¡Œæ™‚é–“: {execution_time:.1f}ç§’)")
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        self.cache.save_cache(operation, result, execution_time)
        
        return result
    
    def _perform_actual_analysis(self) -> Dict:
        """å®Ÿéš›ã®è§£æå‡¦ç†ï¼ˆãƒ‡ãƒ¢ç”¨ï¼‰"""
        time.sleep(2)  # è§£æå‡¦ç†ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        
        return {
            'project_type': 'hierarchical_agent_system',
            'version': 'v8.7',
            'components': {
                'agents': ['CTO', 'å“è³ªä¿è¨¼éƒ¨', 'äººäº‹éƒ¨', 'çµŒå–¶ä¼ç”»éƒ¨', 'ã‚·ã‚¹ãƒ†ãƒ é–‹ç™ºéƒ¨'],
                'commands': ['/spec', '/analyze', '/requirements', '/design', '/tasks'],
                'features': ['SDD+TDD', 'MCPé€£æº', 'è§£æã‚­ãƒ£ãƒƒã‚·ãƒ¥', 'è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—']
            },
            'statistics': {
                'total_files': 42,
                'lines_of_code': 12345,
                'test_coverage': 85
            }
        }

def demo():
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‡ãƒ¢"""
    print("\n" + "=" * 60)
    print("ğŸ’¾ è§£æã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ  ãƒ‡ãƒ¢")
    print("=" * 60 + "\n")
    
    analyzer = CachedAnalyzer()
    
    # 1å›ç›®ã®è§£æï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—ï¼‰
    print("ã€1å›ç›®ã®è§£æã€‘")
    result1 = analyzer.analyze_project()
    print(f"çµæœ: {result1['project_type']}")
    
    print("\n" + "-" * 40 + "\n")
    
    # 2å›ç›®ã®è§£æï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚ã‚Šï¼‰
    print("ã€2å›ç›®ã®è§£æã€‘")
    result2 = analyzer.analyze_project()
    print(f"çµæœ: {result2['project_type']}")
    
    print("\n" + "-" * 40 + "\n")
    
    # çµ±è¨ˆè¡¨ç¤º
    stats = analyzer.cache.get_statistics()
    print("ğŸ“Š ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆ:")
    print(f"  ãƒ’ãƒƒãƒˆç‡: {stats['hit_rate']:.1f}%")
    print(f"  ç·ãƒªã‚¯ã‚¨ã‚¹ãƒˆ: {stats['total_requests']}")
    print(f"  ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: {stats['cache_hits']}")
    print(f"  ç¯€ç´„æ™‚é–“: {stats['time_saved']:.1f}ç§’")
    
    print("\nâœ… ãƒ‡ãƒ¢å®Œäº†\n")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    import argparse
    parser = argparse.ArgumentParser(description='è§£æã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ ')
    parser.add_argument('command', nargs='?', default='demo',
                      choices=['demo', 'clear', 'cleanup', 'stats'])
    parser.add_argument('--force', action='store_true', help='ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡è¦–ã—ã¦è§£æ')
    
    args = parser.parse_args()
    
    if args.command == 'demo':
        demo()
    elif args.command == 'clear':
        cache = AnalysisCache()
        cache.clear_all_cache()
    elif args.command == 'cleanup':
        cache = AnalysisCache()
        cache.cleanup_old_cache()
    elif args.command == 'stats':
        cache = AnalysisCache()
        stats = cache.get_statistics()
        print("ğŸ“Š ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆ:")
        print(f"  ãƒ’ãƒƒãƒˆç‡: {stats['hit_rate']:.1f}%")
        print(f"  ç·ãƒªã‚¯ã‚¨ã‚¹ãƒˆ: {stats['total_requests']}")
        print(f"  ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: {stats['cache_hits']}")
        print(f"  ç¯€ç´„æ™‚é–“: {stats['time_saved']:.1f}ç§’")

if __name__ == "__main__":
    main()