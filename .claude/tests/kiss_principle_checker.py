#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
KISSåŸå‰‡ãƒã‚§ãƒƒã‚«ãƒ¼ - Phase 3ãƒ†ã‚¹ãƒˆæˆ¦ç•¥çµ±ä¸€ç‰ˆ
Keep It Simple, Stupid åŸå‰‡ã®é©ç”¨çŠ¶æ³ã‚’ç›£è¦–ãƒ»è©•ä¾¡

KISSåŸå‰‡ãƒã‚§ãƒƒã‚¯é …ç›®:
1. ãƒ†ã‚¹ãƒˆãƒ¡ã‚½ãƒƒãƒ‰ã®è¤‡é›‘åº¦
2. ãƒ•ã‚¡ã‚¤ãƒ«è¡Œæ•°ã®é©åˆ‡æ€§  
3. ãƒ¡ã‚½ãƒƒãƒ‰æ•°ã®é©åˆ‡æ€§
4. å‘½åè¦å‰‡ã®æ˜ç¢ºæ€§
5. ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³æ•°ã®é©åˆ‡æ€§
"""

import ast
import os
import re
import json
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from enum import Enum


class KISSViolationType(Enum):
    """KISSåŸå‰‡é•åã‚¿ã‚¤ãƒ—"""
    TOO_COMPLEX = "too_complex"              # è¤‡é›‘ã™ãã‚‹
    TOO_LONG = "too_long"                    # é•·ã™ãã‚‹
    TOO_MANY_METHODS = "too_many_methods"    # ãƒ¡ã‚½ãƒƒãƒ‰æ•°éå¤š
    UNCLEAR_NAMING = "unclear_naming"        # ä¸æ˜ç¢ºãªå‘½å
    TOO_MANY_ASSERTIONS = "too_many_assertions"  # ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³éå¤š


@dataclass
class KISSViolation:
    """KISSåŸå‰‡é•åæƒ…å ±"""
    violation_type: KISSViolationType
    file_path: str
    line_number: int
    method_name: str
    description: str
    current_value: Any
    recommended_value: Any
    severity: str = "medium"  # low, medium, high


@dataclass 
class KISSMetrics:
    """KISSåŸå‰‡ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    file_path: str
    total_lines: int = 0
    method_count: int = 0
    max_method_lines: int = 0
    max_method_complexity: int = 0
    average_method_lines: float = 0.0
    violations: List[KISSViolation] = field(default_factory=list)
    
    @property
    def compliance_score(self) -> float:
        """KISSæº–æ‹ ã‚¹ã‚³ã‚¢ (0-100)"""
        if not self.violations:
            return 100.0
            
        # é•åã®é‡ã¿ã¥ã‘
        severity_weights = {"low": 1, "medium": 3, "high": 5}
        total_penalty = sum(severity_weights[v.severity] for v in self.violations)
        
        # ã‚¹ã‚³ã‚¢è¨ˆç®— (100ã‹ã‚‰é•åãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’å¼•ã)
        score = max(0, 100 - (total_penalty * 2))
        return score


class KISSPrincipleChecker:
    """
    KISSåŸå‰‡ãƒã‚§ãƒƒã‚«ãƒ¼
    
    ã‚·ãƒ³ãƒ—ãƒ«ã§æ˜ç¢ºãªãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰ã‚’ç¶­æŒã™ã‚‹ãŸã‚ã®å“è³ªç›£è¦–ãƒ„ãƒ¼ãƒ«
    """
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        # KISSåŸå‰‡ã®åŸºæº–å€¤
        self.standards = {
            "max_file_lines": 500,           # 1ãƒ•ã‚¡ã‚¤ãƒ«æœ€å¤§è¡Œæ•°
            "max_method_lines": 30,          # 1ãƒ¡ã‚½ãƒƒãƒ‰æœ€å¤§è¡Œæ•°  
            "max_methods_per_class": 20,     # 1ã‚¯ãƒ©ã‚¹æœ€å¤§ãƒ¡ã‚½ãƒƒãƒ‰æ•°
            "max_assertions_per_test": 5,    # 1ãƒ†ã‚¹ãƒˆæœ€å¤§ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³æ•°
            "max_cyclomatic_complexity": 10  # å¾ªç’°è¤‡é›‘åº¦
        }
        
        # æ˜ç¢ºãªå‘½åãƒ‘ã‚¿ãƒ¼ãƒ³
        self.clear_naming_patterns = {
            "test_methods": r"^test_[a-z_]+_(red|green|refactor|[a-z_]+)$",
            "helper_methods": r"^_[a-z_]+$",
            "setup_teardown": r"^(setUp|tearDown|setUpClass|tearDownClass)$"
        }
        
    def check_directory(self, directory_path: str) -> Dict[str, KISSMetrics]:
        """ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’KISSåŸå‰‡ã§ãƒã‚§ãƒƒã‚¯"""
        results = {}
        
        test_files = self._find_test_files(directory_path)
        
        for test_file in test_files:
            metrics = self._check_file(test_file)
            results[str(test_file)] = metrics
            
        return results
        
    def _find_test_files(self, directory_path: str) -> List[Path]:
        """ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢"""
        directory = Path(directory_path)
        
        if not directory.exists():
            return []
            
        # test_*.py ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        test_files = []
        for pattern in ["test_*.py", "*_test.py"]:
            test_files.extend(directory.rglob(pattern))
            
        return test_files
        
    def _check_file(self, file_path: Path) -> KISSMetrics:
        """å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®KISSåŸå‰‡ãƒã‚§ãƒƒã‚¯"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                lines = content.split('\n')
                
        except Exception as e:
            # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼
            return KISSMetrics(
                file_path=str(file_path),
                violations=[KISSViolation(
                    violation_type=KISSViolationType.TOO_COMPLEX,
                    file_path=str(file_path),
                    line_number=0,
                    method_name="<file>",
                    description=f"File read error: {e}",
                    current_value=str(e),
                    recommended_value="Fix file encoding or permissions",
                    severity="high"
                )]
            )
            
        metrics = KISSMetrics(
            file_path=str(file_path),
            total_lines=len(lines)
        )
        
        # ASTã‚’ä½¿ç”¨ã—ãŸã‚³ãƒ¼ãƒ‰è§£æ
        try:
            tree = ast.parse(content)
            self._analyze_ast(tree, metrics, lines)
            
        except SyntaxError as e:
            metrics.violations.append(KISSViolation(
                violation_type=KISSViolationType.TOO_COMPLEX,
                file_path=str(file_path),
                line_number=e.lineno or 0,
                method_name="<syntax>",
                description=f"Syntax error: {e.msg}",
                current_value="Syntax error",
                recommended_value="Fix syntax errors",
                severity="high"
            ))
            
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¬ãƒ™ãƒ«ã®ãƒã‚§ãƒƒã‚¯
        self._check_file_complexity(metrics)
        
        return metrics
        
    def _analyze_ast(self, tree: ast.AST, metrics: KISSMetrics, lines: List[str]):
        """ASTè§£æã«ã‚ˆã‚‹KISSåŸå‰‡ãƒã‚§ãƒƒã‚¯"""
        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef):
                self._analyze_class(node, metrics, lines)
            elif isinstance(node, ast.FunctionDef):
                self._analyze_function(node, metrics, lines)
                
    def _analyze_class(self, class_node: ast.ClassDef, metrics: KISSMetrics, lines: List[str]):
        """ã‚¯ãƒ©ã‚¹è§£æ"""
        # ãƒ¡ã‚½ãƒƒãƒ‰æ•°ã‚«ã‚¦ãƒ³ãƒˆ
        methods = [n for n in class_node.body if isinstance(n, ast.FunctionDef)]
        method_count = len(methods)
        metrics.method_count = method_count
        
        # ãƒ¡ã‚½ãƒƒãƒ‰æ•°ãƒã‚§ãƒƒã‚¯
        if method_count > self.standards["max_methods_per_class"]:
            metrics.violations.append(KISSViolation(
                violation_type=KISSViolationType.TOO_MANY_METHODS,
                file_path=metrics.file_path,
                line_number=class_node.lineno,
                method_name=class_node.name,
                description=f"Class has too many methods ({method_count})",
                current_value=method_count,
                recommended_value=self.standards["max_methods_per_class"],
                severity="medium"
            ))
            
        # å„ãƒ¡ã‚½ãƒƒãƒ‰ã®è§£æ
        method_lines = []
        for method in methods:
            method_line_count = self._count_method_lines(method, lines)
            method_lines.append(method_line_count)
            
            # ãƒ¡ã‚½ãƒƒãƒ‰è¡Œæ•°ãƒã‚§ãƒƒã‚¯
            if method_line_count > self.standards["max_method_lines"]:
                metrics.violations.append(KISSViolation(
                    violation_type=KISSViolationType.TOO_LONG,
                    file_path=metrics.file_path,
                    line_number=method.lineno,
                    method_name=method.name,
                    description=f"Method too long ({method_line_count} lines)",
                    current_value=method_line_count,
                    recommended_value=self.standards["max_method_lines"],
                    severity="medium"
                ))
                
            # å‘½åè¦å‰‡ãƒã‚§ãƒƒã‚¯
            self._check_method_naming(method, metrics)
            
            # ãƒ†ã‚¹ãƒˆãƒ¡ã‚½ãƒƒãƒ‰ã®å ´åˆã¯ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³æ•°ãƒã‚§ãƒƒã‚¯
            if method.name.startswith('test_'):
                self._check_test_method_assertions(method, metrics)
                
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
        if method_lines:
            metrics.max_method_lines = max(method_lines)
            metrics.average_method_lines = sum(method_lines) / len(method_lines)
            
    def _analyze_function(self, func_node: ast.FunctionDef, metrics: KISSMetrics, lines: List[str]):
        """é–¢æ•°è§£æï¼ˆã‚¯ãƒ©ã‚¹å¤–ã®é–¢æ•°ï¼‰"""
        method_line_count = self._count_method_lines(func_node, lines)
        
        if method_line_count > self.standards["max_method_lines"]:
            metrics.violations.append(KISSViolation(
                violation_type=KISSViolationType.TOO_LONG,
                file_path=metrics.file_path,
                line_number=func_node.lineno,
                method_name=func_node.name,
                description=f"Function too long ({method_line_count} lines)",
                current_value=method_line_count,
                recommended_value=self.standards["max_method_lines"],
                severity="medium"
            ))
            
    def _count_method_lines(self, method_node: ast.FunctionDef, lines: List[str]) -> int:
        """ãƒ¡ã‚½ãƒƒãƒ‰ã®å®Ÿéš›ã®è¡Œæ•°ã‚’è¨ˆç®—ï¼ˆç©ºè¡Œãƒ»ã‚³ãƒ¡ãƒ³ãƒˆé™¤ãï¼‰"""
        start_line = method_node.lineno - 1  # 0-based index
        
        # ãƒ¡ã‚½ãƒƒãƒ‰ã®çµ‚äº†è¡Œã‚’è¦‹ã¤ã‘ã‚‹
        end_line = start_line
        for i, line in enumerate(lines[start_line:], start_line):
            if line.strip() and not line.startswith(' ') and not line.startswith('\t') and i > start_line:
                break
            end_line = i
            
        # å®Ÿéš›ã®ã‚³ãƒ¼ãƒ‰è¡Œæ•°ï¼ˆç©ºè¡Œã¨ã‚³ãƒ¡ãƒ³ãƒˆè¡Œã‚’é™¤ãï¼‰
        code_lines = 0
        for i in range(start_line, min(end_line + 1, len(lines))):
            line = lines[i].strip()
            if line and not line.startswith('#') and not line.startswith('"""') and not line.startswith("'''"):
                code_lines += 1
                
        return code_lines
        
    def _check_file_complexity(self, metrics: KISSMetrics):
        """ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¬ãƒ™ãƒ«ã®è¤‡é›‘åº¦ãƒã‚§ãƒƒã‚¯"""
        if metrics.total_lines > self.standards["max_file_lines"]:
            metrics.violations.append(KISSViolation(
                violation_type=KISSViolationType.TOO_LONG,
                file_path=metrics.file_path,
                line_number=0,
                method_name="<file>",
                description=f"File too long ({metrics.total_lines} lines)",
                current_value=metrics.total_lines,
                recommended_value=self.standards["max_file_lines"],
                severity="high"
            ))
            
    def _check_method_naming(self, method_node: ast.FunctionDef, metrics: KISSMetrics):
        """ãƒ¡ã‚½ãƒƒãƒ‰å‘½åè¦å‰‡ãƒã‚§ãƒƒã‚¯"""
        method_name = method_node.name
        
        # ãƒ†ã‚¹ãƒˆãƒ¡ã‚½ãƒƒãƒ‰ã®å‘½åãƒã‚§ãƒƒã‚¯
        if method_name.startswith('test_'):
            pattern = self.clear_naming_patterns["test_methods"]
            if not re.match(pattern, method_name):
                metrics.violations.append(KISSViolation(
                    violation_type=KISSViolationType.UNCLEAR_NAMING,
                    file_path=metrics.file_path,
                    line_number=method_node.lineno,
                    method_name=method_name,
                    description="Test method naming not clear (should include red/green/refactor or be descriptive)",
                    current_value=method_name,
                    recommended_value="test_feature_description_[red|green|refactor]",
                    severity="low"
                ))
                
    def _check_test_method_assertions(self, method_node: ast.FunctionDef, metrics: KISSMetrics):
        """ãƒ†ã‚¹ãƒˆãƒ¡ã‚½ãƒƒãƒ‰ã®ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³æ•°ãƒã‚§ãƒƒã‚¯"""
        assertion_count = 0
        
        for node in ast.walk(method_node):
            if isinstance(node, ast.Call):
                if hasattr(node.func, 'attr') and node.func.attr.startswith('assert'):
                    assertion_count += 1
                elif hasattr(node.func, 'id') and node.func.id.startswith('assert'):
                    assertion_count += 1
                    
        if assertion_count > self.standards["max_assertions_per_test"]:
            metrics.violations.append(KISSViolation(
                violation_type=KISSViolationType.TOO_MANY_ASSERTIONS,
                file_path=metrics.file_path,
                line_number=method_node.lineno,
                method_name=method_node.name,
                description=f"Too many assertions in test method ({assertion_count})",
                current_value=assertion_count,
                recommended_value=self.standards["max_assertions_per_test"],
                severity="medium"
            ))
            
    def generate_report(self, results: Dict[str, KISSMetrics]) -> Dict[str, Any]:
        """KISSåŸå‰‡ãƒã‚§ãƒƒã‚¯çµæœã®ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        if not results:
            return {
                "status": "No test files found",
                "overall_compliance": 100.0,
                "files_checked": 0,
                "total_violations": 0
            }
            
        # å…¨ä½“çµ±è¨ˆ
        total_files = len(results)
        total_violations = sum(len(metrics.violations) for metrics in results.values())
        compliance_scores = [metrics.compliance_score for metrics in results.values()]
        overall_compliance = sum(compliance_scores) / len(compliance_scores) if compliance_scores else 0.0
        
        # é•åã‚¿ã‚¤ãƒ—åˆ¥é›†è¨ˆ
        violation_types = {}
        severity_counts = {"low": 0, "medium": 0, "high": 0}
        
        for metrics in results.values():
            for violation in metrics.violations:
                violation_type = violation.violation_type.value
                violation_types[violation_type] = violation_types.get(violation_type, 0) + 1
                severity_counts[violation.severity] += 1
                
        # æ¨å¥¨äº‹é …
        recommendations = self._generate_recommendations(violation_types, severity_counts)
        
        return {
            "overall_compliance": round(overall_compliance, 1),
            "status": "ğŸŸ¢ EXCELLENT" if overall_compliance >= 90 else 
                     "ğŸŸ¡ GOOD" if overall_compliance >= 70 else 
                     "ğŸ”´ NEEDS IMPROVEMENT",
            "files_checked": total_files,
            "total_violations": total_violations,
            "violation_breakdown": violation_types,
            "severity_breakdown": severity_counts,
            "recommendations": recommendations,
            "file_details": {
                file_path: {
                    "compliance_score": metrics.compliance_score,
                    "violations": len(metrics.violations),
                    "total_lines": metrics.total_lines,
                    "method_count": metrics.method_count,
                    "max_method_lines": metrics.max_method_lines
                }
                for file_path, metrics in results.items()
            }
        }
        
    def _generate_recommendations(self, violation_types: Dict[str, int], 
                                severity_counts: Dict[str, int]) -> List[str]:
        """æ”¹å–„æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = []
        
        if violation_types.get("too_long", 0) > 0:
            recommendations.append("ğŸ“ Split long methods and files into smaller, focused units")
            
        if violation_types.get("too_many_methods", 0) > 0:
            recommendations.append("ğŸ”„ Consider splitting large test classes by functionality")
            
        if violation_types.get("unclear_naming", 0) > 0:
            recommendations.append("ğŸ“ Use clear, descriptive naming with red/green/refactor indicators")
            
        if violation_types.get("too_many_assertions", 0) > 0:
            recommendations.append("ğŸ¯ Focus each test method on a single behavior (one concept per test)")
            
        if severity_counts["high"] > 0:
            recommendations.append("ğŸš¨ Address high-severity violations first for maximum impact")
            
        if not recommendations:
            recommendations.append("âœ… Excellent KISS principle compliance! Keep up the simple, clear code.")
            
        return recommendations


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='KISS Principle Checker for Tests')
    parser.add_argument('directory', nargs='?', default='.', 
                       help='Directory to check (default: current directory)')
    parser.add_argument('--output', '-o', choices=['console', 'json'], default='console',
                       help='Output format')
    parser.add_argument('--strict', action='store_true', 
                       help='Use stricter standards')
    
    args = parser.parse_args()
    
    # Strictãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯åŸºæº–ã‚’å³ã—ã
    checker = KISSPrincipleChecker()
    if args.strict:
        checker.standards.update({
            "max_file_lines": 300,
            "max_method_lines": 20,
            "max_methods_per_class": 15,
            "max_assertions_per_test": 3
        })
        
    # ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
    results = checker.check_directory(args.directory)
    report = checker.generate_report(results)
    
    # çµæœå‡ºåŠ›
    if args.output == 'json':
        print(json.dumps(report, indent=2, ensure_ascii=False))
    else:
        # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›
        print("=" * 80)
        print("KISSåŸå‰‡ãƒã‚§ãƒƒã‚¯çµæœ")  
        print("=" * 80)
        print(f"ç·åˆæº–æ‹ ç‡: {report['overall_compliance']}% {report['status']}")
        print(f"ãƒã‚§ãƒƒã‚¯ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {report['files_checked']}")
        print(f"é•åç·æ•°: {report['total_violations']}")
        
        if report['violation_breakdown']:
            print("\né•åã‚¿ã‚¤ãƒ—åˆ¥:")
            for vtype, count in report['violation_breakdown'].items():
                print(f"  {vtype}: {count}ä»¶")
                
        if report['recommendations']:
            print("\næ¨å¥¨äº‹é …:")
            for rec in report['recommendations']:
                print(f"  {rec}")
                
        print("=" * 80)


if __name__ == "__main__":
    main()